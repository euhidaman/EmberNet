# EmberNet - Tiny BitNet MoE Vision-Language Model

A high-quality, tiny BitNet b1.58-style Mixture-of-Experts Vision-Language Model (~300M ternary parameters) designed for edge deployment.

**What it does:** You give it an image + a question, it gives you an intelligent answer. OCR, chart analysis, document understanding, visual reasoning - all in <500MB.

---

## Table of Contents

1. [Quick Start](#quick-start)
2. [How It Works](#how-it-works)
3. [Complete Dataset List](#complete-dataset-list)
4. [Training Guide](#training-guide)
5. [Usage Examples](#usage-examples)
6. [Architecture Details](#architecture-details)

---

## Quick Start

### Complete Setup & Training Guide

Follow these steps in order to set up and train EmberNet:

#### Step 1: Install Dependencies

```bash
# Navigate to project directory
cd EmberNet

# Install all required packages
pip install -r requirements.txt
```

**Required packages:**
- `torch>=2.0.0` - Deep learning framework
- `transformers>=4.36.0` - HuggingFace transformers (for SigLIP)
- `datasets>=2.14.0` - HuggingFace datasets
- `wandb>=0.16.0` - Experiment tracking
- `huggingface_hub>=0.19.0` - HuggingFace authentication
- `Pillow>=9.0.0` - Image processing
- `einops>=0.7.0` - Tensor operations
- `numpy>=1.24.0` - Numerical computing

---

#### Step 2: Authentication Setup

**2a. HuggingFace Login** (REQUIRED - for downloading datasets):
```bash
# Login to HuggingFace
huggingface-cli login

# When prompted, enter your HuggingFace token
# Get your token from: https://huggingface.co/settings/tokens
# Create a token with "read" permissions
```

**2b. Weights & Biases Login** (OPTIONAL - for experiment tracking):
```bash
# Login to W&B
wandb login

# When prompted, enter your W&B API key
# Get your API key from: https://wandb.ai/authorize
# This enables training visualization and metric tracking
```

---

#### Step 3: Download Training Datasets

**Choose one option based on your needs:**

**Option A: All Datasets (~100GB)** - Best quality, all datasets
```bash
python training/prepare_data.py --all --output-dir ./data
```
Downloads all 20 datasets for maximum model quality.

**Option B: Recommended (~70GB)** - Good balance of quality and size
```bash
python training/prepare_data.py --recommended --output-dir ./data
```
Downloads critical + recommended datasets (excludes optional ones).

**Option C: Critical Only (~50GB)** - Core datasets only
```bash
python training/prepare_data.py --critical --output-dir ./data
```
Downloads only the most important datasets.

**Option D: Minimal (~10GB)** - Quick testing
```bash
python training/prepare_data.py --minimal --output-dir ./data
```
Downloads a tiny subset for testing the pipeline.

**Other useful commands:**
```bash
# List all available datasets before downloading
python training/prepare_data.py --list

# Download specific datasets only
python training/prepare_data.py --dataset textvqa chartqa vqav2 --output-dir ./data

# Explain how alignment works
python training/prepare_data.py --explain
```

**What gets downloaded:**
- Stage 1 datasets: LLaVA-Instruct, ShareGPT4V, ALLaVA, COCO Captions, etc.
- Stage 2 datasets: TextVQA, DocVQA, AI2D, ChartQA, PlotQA, VQAv2, GQA, OK-VQA, A-OKVQA, ScienceQA, RefCOCO, NLVR2, VSR, and more

**Note:** Some datasets (ShareGPT4V, ALLaVA, ChartQA, GQA, VSR) download images from URLs, which may take longer than loading pre-packaged datasets.

**What gets saved:**
After downloading, you'll have:
- `./data/{dataset_name}/` - Each dataset in its own folder
- `./data/download_manifest.json` - **Central tracking file** with all download info
- `./data/dataset_index.json` - Index mapping datasets to stages/domains
- `./data/{dataset_name}/metadata.json` - Individual dataset metadata

The **download_manifest.json** tracks:
- All download sessions with timestamps
- Dataset metadata (samples, size, domain, expert)
- Download times and success/failure status
- File paths and HuggingFace IDs

#### Step 4: Train the Model

EmberNet uses a **two-stage training pipeline**. Use `--trial` for quick validation or `--main` for full training.

---

**ğŸš€ Quick Start: Single-Command Training**

```bash
# Trial Run - Quick pipeline validation (minutes, not hours)
python training/train.py --trial --data-dir ./data

# Main Run - Full production training (hours/days)
python training/train.py --main --data-dir ./data
```

Both commands automatically run **Stage 1 â†’ Stage 2** sequentially.

---

**Trial Mode (`--trial`)**

Validates the entire pipeline with minimal data:

```bash
python training/train.py --trial --data-dir ./data
```

| Setting | Value |
|---------|-------|
| Samples per dataset | 50 |
| Epochs per stage | 1 |
| Batch size | 2 |
| Gradient accumulation | 1 |
| W&B logging | Disabled |
| Output | `./checkpoints/trial/stage{1,2}/` |

Use this to verify everything works before committing to full training.

---

**Main Mode (`--main`)**

Full production training with all data:

```bash
python training/train.py --main --data-dir ./data
```

| Setting | Stage 1 | Stage 2 |
|---------|---------|---------|
| Samples | ALL | ALL |
| Epochs | 3 | 10 |
| Batch size | 8 | 4 |
| Gradient accumulation | 4 | 4 |
| W&B logging | Enabled | Enabled |
| Output | `./checkpoints/stage1/` | `./checkpoints/stage2/` |

---

**Run a Specific Stage Only**

```bash
# Stage 1 only (projector alignment)
python training/train.py --trial --stage 1 --data-dir ./data

# Stage 2 only (expert specialization) - requires Stage 1 checkpoint
python training/train.py --trial --stage 2 --data-dir ./data \
    --resume ./checkpoints/trial/stage1/final_model.pt
```

---

**Training Options**

```bash
# Custom epochs and batch size
python training/train.py --main --data-dir ./data --epochs 5 --batch-size 4

# Limit samples per dataset (useful for debugging)
python training/train.py --main --data-dir ./data --max-samples-per-dataset 1000

# Disable specific features
python training/train.py --main --data-dir ./data \
    --no-wandb \          # Disable W&B logging
    --no-ema \            # Disable EMA
    --no-curriculum \     # Disable curriculum learning
    --no-adaptive-clip    # Disable adaptive gradient clipping

# Hardware adjustments
python training/train.py --main --data-dir ./data \
    --device cpu \        # Force CPU (slower)
    --no-amp \            # Disable mixed precision
    --num-workers 2       # Reduce data loading workers
```

---

**Output Structure**

After training completes:

```
checkpoints/
â”œâ”€â”€ trial/                    # Trial mode outputs
â”‚   â”œâ”€â”€ stage1/
â”‚   â”‚   â”œâ”€â”€ final_model.pt    # Stage 1 checkpoint
â”‚   â”‚   â””â”€â”€ checkpoint_epoch_1.pt
â”‚   â””â”€â”€ stage2/
â”‚       â”œâ”€â”€ final_model.pt    # Stage 2 checkpoint (final model)
â”‚       â””â”€â”€ checkpoint_epoch_1.pt
â”‚
â”œâ”€â”€ stage1/                   # Main mode outputs
â”‚   â”œâ”€â”€ final_model.pt
â”‚   â”œâ”€â”€ best_model.pt
â”‚   â””â”€â”€ checkpoint_epoch_*.pt
â”‚
â””â”€â”€ stage2/
    â”œâ”€â”€ final_model.pt        # â† Use this for inference
    â”œâ”€â”€ best_model.pt
    â””â”€â”€ checkpoint_epoch_*.pt
```

---

**What Each Stage Does**

| Stage | Purpose | What Trains | What's Frozen |
|-------|---------|-------------|---------------|
| **Stage 1** | Vision-Language Alignment | CrossModal Projector, Pooler, Compressor | Vision Encoder, LM Decoder |
| **Stage 2** | Expert Specialization | MoE Router, Domain Experts | Vision Encoder, Projector, Embeddings |

---

#### Step 5: Convert Model (Optional)

Convert to optimized ternary format for deployment:

```bash
python inference/convert.py \
    ./checkpoints/stage2/final_model.pt \
    ./embernet_optimized.pt
```

This packs ternary weights to 2-bit representation, reducing model size to <500MB.

---

#### Step 6: Run Inference

**Interactive Mode:**
```bash
python inference/infer.py \
    --model ./checkpoints/stage2/final_model.pt \
    --interactive
```

**Interactive commands:**
- `/load image.jpg` - Load an image
- `/describe` - Describe the image
- `/ocr` - Extract text from image
- `/chart` - Analyze chart/graph
- `/clear` - Reset conversation
- `/quit` - Exit

**Single Query:**
```bash
python inference/infer.py \
    --model ./checkpoints/stage2/final_model.pt \
    --image photo.jpg \
    --prompt "What's in this image?"
```

---

### Quick Reference: All Commands in Order

```bash
# 1. Install dependencies
cd EmberNet
pip install -r requirements.txt

# 2. Authenticate (HF required, W&B optional)
huggingface-cli login   # Enter token from https://huggingface.co/settings/tokens
wandb login             # Optional: Enter API key from https://wandb.ai/authorize

# 3. Download training data
python training/prepare_data.py --recommended --output-dir ./data   # ~70GB
# OR: python training/prepare_data.py --all --output-dir ./data     # ~100GB (best quality)
# OR: python training/prepare_data.py --minimal --output-dir ./data # ~10GB (testing only)

# 4. Train the model (SINGLE COMMAND - runs Stage 1 â†’ Stage 2 automatically)
python training/train.py --trial --data-dir ./data   # Quick test (~minutes)
# OR:
python training/train.py --main --data-dir ./data    # Full training (~hours/days)

# 5. Run interactive inference
python inference/infer.py --model ./checkpoints/stage2/final_model.pt --interactive
```

---

## How It Works

### Vision-Language Alignment Explained

EmberNet connects images to language through a carefully designed pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           IMAGE INPUT                                    â”‚
â”‚                              â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. VISION ENCODER (SigLIP - Frozen)                            â”‚    â”‚
â”‚  â”‚    â€¢ Pretrained on 400M image-text pairs from Google           â”‚    â”‚
â”‚  â”‚    â€¢ Extracts 196 visual tokens (14Ã—14 grid)                   â”‚    â”‚
â”‚  â”‚    â€¢ Each token is a 768-dimensional feature vector            â”‚    â”‚
â”‚  â”‚    â€¢ Already "understands" visual concepts                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 2. TOKEN COMPRESSION (Trainable)                               â”‚    â”‚
â”‚  â”‚    â€¢ Pixel Shuffle: 196 â†’ 49 tokens (merges 2Ã—2 neighbors)     â”‚    â”‚
â”‚  â”‚    â€¢ Adaptive Pooling: 49 â†’ 64 tokens (learned queries)        â”‚    â”‚
â”‚  â”‚    â€¢ Preserves important visual information                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 3. PROJECTOR (BitLinear MLP) â† THIS IS WHERE ALIGNMENT HAPPENS â”‚    â”‚
â”‚  â”‚    â€¢ 2-layer MLP with ternary weights {-1, 0, +1}              â”‚    â”‚
â”‚  â”‚    â€¢ Maps: Vision embedding space â†’ Language embedding space   â”‚    â”‚
â”‚  â”‚    â€¢ Trained in Stage 1 on image-caption pairs                 â”‚    â”‚
â”‚  â”‚    â€¢ After training, visual tokens "look like" word tokens     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 4. MERGED INPUT SEQUENCE                                       â”‚    â”‚
â”‚  â”‚    [BOS] [IMGâ‚] [IMGâ‚‚] ... [IMGâ‚†â‚„] [User: Describe this] ...  â”‚    â”‚
â”‚  â”‚          â””â”€â”€ visual tokens â”€â”€â”˜     â””â”€â”€ text tokens â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚    The LLM processes both as if they're all "text"             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 5. BITNET MOE DECODER                                          â”‚    â”‚
â”‚  â”‚    â€¢ 16 transformer layers with ternary weights                â”‚    â”‚
â”‚  â”‚    â€¢ MoE FFN: 8 specialized experts + 1 shared expert          â”‚    â”‚
â”‚  â”‚    â€¢ Router sends tokens to relevant experts:                  â”‚    â”‚
â”‚  â”‚      - OCR expert for text reading                             â”‚    â”‚
â”‚  â”‚      - Chart expert for graphs                                 â”‚    â”‚
â”‚  â”‚      - Diagram expert for technical drawings                   â”‚    â”‚
â”‚  â”‚      - etc.                                                    â”‚    â”‚
â”‚  â”‚    â€¢ Generates response token by token                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                          â”‚
â”‚                        TEXT OUTPUT                                      â”‚
â”‚            "This image shows a bar chart comparing..."                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Two Training Stages?

**Stage 1 - Projector Alignment:**
- Goal: Teach the model to "see" by connecting visual features to language
- What's trained: Projector MLP + Token compression layers
- What's frozen: Vision encoder + Language decoder
- Data: High-quality image descriptions (LLaVA-Instruct, ShareGPT4V)
- Why: The projector needs to learn that "this visual pattern" = "the concept of a dog"

**Stage 2 - Expert Specialization:**
- Goal: Make experts good at specific tasks (OCR, charts, diagrams, etc.)
- What's trained: MoE experts + Router
- What's frozen: Vision encoder + Embeddings
- Data: Domain-specific datasets (TextVQA, ChartQA, DocVQA, etc.)
- Why: Different visual tasks need different skills - one expert can't do everything well

### Expert Architecture & Dataset Mapping

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EMBERNET MoE EXPERT ARCHITECTURE                          â”‚
â”‚                                                                              â”‚
â”‚  ROUTING: Each token â†’ TOP-2 Experts + SHARED Expert (always active)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  EXPERT 0: vision_ocr                                                        â”‚
â”‚  â”œâ”€ Specialty: Read text in images, OCR, document parsing                   â”‚
â”‚  â””â”€ Datasets: TextVQA, DocVQA, OCR-VQA                                      â”‚
â”‚                                                                              â”‚
â”‚  EXPERT 1: vision_diagram                                                    â”‚
â”‚  â”œâ”€ Specialty: Understand diagrams, infographics, technical drawings        â”‚
â”‚  â””â”€ Datasets: AI2D, InfoVQA                                                 â”‚
â”‚                                                                              â”‚
â”‚  EXPERT 2: code_math_chart                                                   â”‚
â”‚  â”œâ”€ Specialty: Analyze charts, graphs, plots, data visualizations           â”‚
â”‚  â””â”€ Datasets: ChartQA, PlotQA, FigureQA, DVQA                              â”‚
â”‚                                                                              â”‚
â”‚  EXPERT 3: code_math_formula                                                 â”‚
â”‚  â”œâ”€ Specialty: Handle math equations, formulas, numerical reasoning         â”‚
â”‚  â””â”€ Datasets: MathVista                                                     â”‚
â”‚                                                                              â”‚
â”‚  EXPERT 4: spatial_scene                                                     â”‚
â”‚  â”œâ”€ Specialty: Scene understanding, object detection, descriptions          â”‚
â”‚  â””â”€ Datasets: VQAv2, Visual Genome                                          â”‚
â”‚                                                                              â”‚
â”‚  EXPERT 5: spatial_reasoning                                                 â”‚
â”‚  â”œâ”€ Specialty: Spatial relationships, counting, positional reasoning        â”‚
â”‚  â””â”€ Datasets: GQA                                                           â”‚
â”‚                                                                              â”‚
â”‚  EXPERT 6: agentic_knowledge                                                 â”‚
â”‚  â”œâ”€ Specialty: Knowledge-based QA, facts requiring world knowledge          â”‚
â”‚  â””â”€ Datasets: OK-VQA, A-OKVQA                                               â”‚
â”‚                                                                              â”‚
â”‚  EXPERT 7: agentic_reasoning                                                 â”‚
â”‚  â”œâ”€ Specialty: Multi-step reasoning, logic, science questions               â”‚
â”‚  â””â”€ Datasets: ScienceQA, CLEVR                                              â”‚
â”‚                                                                              â”‚
â”‚  SHARED EXPERT (Always Active)                                               â”‚
â”‚  â”œâ”€ Specialty: Common patterns, language generation, general knowledge      â”‚
â”‚  â””â”€ Datasets: ALL datasets (learns shared representations)                  â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example Routing:
  User: "What does this chart show? The title says Q3 Sales."
         â”‚
         â”œâ”€> EXPERT 0 (vision_ocr)    - reads "Q3 Sales" text
         â”œâ”€> EXPERT 2 (chart)          - analyzes chart structure  
         â””â”€> SHARED EXPERT             - general language/context
         
  Combined output: "This bar chart shows Q3 sales data..."
```

---

## Complete Dataset List

### Stage 1: Vision-Language Alignment

| Dataset | HuggingFace ID | Description | Samples | Size |
|---------|----------------|-------------|---------|------|
| **LLaVA-Instruct-150K** â˜… | `lmms-lab/LLaVA-Instruct-150K` | GPT-4 generated visual conversations | 150K | ~5GB |
| **ShareGPT4V** â˜… | `lmms-lab/ShareGPT4V` | Detailed image descriptions from GPT-4V | 100K | ~8GB |
| **ALLaVA** | `FreedomIntelligence/ALLaVA-4V` | Diverse visual instructions | 711K | ~6GB |

### Stage 2: Expert Specialization

#### Vision/OCR Expert Datasets

| Dataset | HuggingFace ID | Description | Samples | Size |
|---------|----------------|-------------|---------|------|
| **TextVQA** â˜… | `lmms-lab/TextVQA` | Text in natural scenes | 45K | ~2GB |
| **DocVQA** â˜… | `lmms-lab/DocVQA` | Documents, forms, receipts | 50K | ~3GB |
| **AI2D** â˜… | `lmms-lab/ai2d` | Scientific diagrams | 15K | ~1.5GB |
| **InfoVQA** | `lmms-lab/InfographicVQA` | Infographics | 30K | ~2.5GB |
| **OCR-VQA** | `howard-hou/OCR-VQA` | Book covers, signs | 200K | ~4GB |

#### Code/Math/Chart Expert Datasets

| Dataset | HuggingFace ID | Description | Samples | Size |
|---------|----------------|-------------|---------|------|
| **ChartQA** â˜… | `ahmed-masry/ChartQA` | Bar, line, pie charts | 32K | ~1GB |
| **MathVista** â˜… | `AI4Math/MathVista` | Mathematical visual reasoning | 6K | ~1GB |
| **PlotQA** | `lmms-lab/PlotQA` | Scientific plots | 224K | ~8GB |
| **FigureQA** | `lmms-lab/FigureQA` | Figure understanding | 180K | ~5GB |
| **DVQA** | `lmms-lab/DVQA` | Data visualization | 300K | ~3GB |

#### Spatial/Scene Expert Datasets

| Dataset | HuggingFace ID | Description | Samples | Size |
|---------|----------------|-------------|---------|------|
| **VQAv2** â˜… | `lmms-lab/VQAv2` | General visual QA | 1.1M | ~25GB |
| **GQA** | `lmms-lab/GQA` | Scene graph reasoning | 22M | ~15GB |
| **Visual Genome** | `lmms-lab/VisualGenome` | Dense scene annotations | 108K | ~15GB |

#### Reasoning Expert Datasets

| Dataset | HuggingFace ID | Description | Samples | Size |
|---------|----------------|-------------|---------|------|
| **ScienceQA** â˜… | `derek-thomas/ScienceQA` | Science with diagrams | 21K | ~2GB |
| **OK-VQA** | `lmms-lab/OK-VQA` | Outside knowledge VQA | 14K | ~1GB |
| **A-OKVQA** | `lmms-lab/A-OKVQA` | Augmented knowledge VQA | 25K | ~1.5GB |
| **CLEVR** | `lmms-lab/CLEVR` | Compositional reasoning | 850K | ~18GB |

**â˜… = Critical (included in --critical mode)**

### Download Options

```bash
# Minimal (~10GB) - For quick testing only
python training/prepare_data.py --minimal
# Includes: llava_instruct_150k, textvqa, chartqa, vqav2

# Critical (~45GB) - Essential for a working model
python training/prepare_data.py --critical
# Includes: All â˜… marked datasets above

# Recommended (~70GB) - Good quality model
python training/prepare_data.py --recommended
# Includes: Critical + recommended datasets

# All (~100GB) - Maximum quality
python training/prepare_data.py --all
# Includes: Everything
```

---

## Training Guide

### Hardware Requirements

| Stage | Minimum | Recommended | Notes |
|-------|---------|-------------|-------|
| Stage 1 | 8GB VRAM | 16GB VRAM | Can use CPU with 16GB RAM (slower) |
| Stage 2 | 12GB VRAM | 24GB VRAM | Can use CPU with 32GB RAM (slower) |

### Expected Training Time

| Stage | GPU (A100) | GPU (RTX 3090) | CPU (32 cores) |
|-------|------------|----------------|----------------|
| Stage 1 (3 epochs) | 6-12 hours | 12-24 hours | 3-5 days |
| Stage 2 (10 epochs) | 24-48 hours | 48-96 hours | 10-15 days |

### Training Commands Reference

**Basic Training (Recommended):**

```bash
# Stage 1: Projector Alignment
python training/train.py \
    --stage 1 \
    --data-dir ./data \
    --epochs 3 \
    --batch-size 8 \
    --output-dir ./checkpoints

# Stage 2: Expert Specialization  
python training/train.py \
    --stage 2 \
    --data-dir ./data \
    --epochs 10 \
    --batch-size 4 \
    --resume ./checkpoints/checkpoint_epoch_3.pt \
    --output-dir ./checkpoints
```

**With W&B Logging (Recommended):**

```bash
# Stage 1
python training/train.py \
    --stage 1 \
    --data-dir ./data \
    --epochs 3 \
    --batch-size 8 \
    --wandb-project EmberNet \
    --wandb-run-name stage1_projector

# Stage 2
python training/train.py \
    --stage 2 \
    --data-dir ./data \
    --epochs 10 \
    --batch-size 4 \
    --resume ./checkpoints/checkpoint_epoch_3.pt \
    --wandb-project EmberNet \
    --wandb-run-name stage2_experts
```

**Custom Learning Rate:**

```bash
python training/train.py \
    --stage 1 \
    --data-dir ./data \
    --epochs 3 \
    --lr 5e-4
```

**Resume from Checkpoint:**

```bash
python training/train.py \
    --stage 2 \
    --data-dir ./data \
    --resume ./checkpoints/checkpoint_epoch_5.pt
```

### Troubleshooting

**Out of Memory (OOM) Errors:**
```bash
# Reduce batch size and increase gradient accumulation
python training/train.py \
    --stage 1 \
    --data-dir ./data \
    --batch-size 2 \
    --grad-accum 16
# Effective batch size = 2 * 16 = 32
```

**Slow Training:**
```bash
# Check if GPU is being used
python training/train.py --stage 1 --data-dir ./data --device cuda

# Increase number of data loading workers
python training/train.py --stage 1 --data-dir ./data --num-workers 8
```

**Mixed Precision Issues:**
```bash
# Disable automatic mixed precision
python training/train.py --stage 1 --data-dir ./data --no-amp
```

**CPU Training:**
```bash
# Force CPU (much slower but works without GPU)
python training/train.py --stage 1 --data-dir ./data --device cpu --batch-size 1
```

### All Training Arguments

```
Training Settings:
  --stage {1,2}              Training stage (1=projector, 2=expert SFT)
  --epochs N                 Number of training epochs (default: 3)
  --batch-size N             Training batch size (default: 4)
  --lr LR                    Learning rate (auto-set: 1e-3 for stage1, 1e-4 for stage2)
  --grad-accum N             Gradient accumulation steps (default: 4)

Paths:
  --data-dir DIR             Data directory (default: ./data)
  --output-dir DIR           Output directory for checkpoints (default: ./checkpoints)
  --resume PATH              Resume from checkpoint

Hardware:
  --device DEVICE            Device: cuda/cpu/auto (default: auto)
  --no-amp                   Disable mixed precision training
  --num-workers N            Data loading workers (default: 4)

Experiment Tracking:
  --wandb                    Use W&B logging (default: True)
  --no-wandb                 Disable W&B logging
  --wandb-project NAME       W&B project name (default: EmberNet)
  --wandb-run-name NAME      W&B run name (default: auto-generated)
```

### What Gets Saved

During training, the following checkpoints are saved to `--output-dir`:

- `checkpoint_epoch_N.pt` - Saved after each epoch
- `checkpoint_step_N.pt` - Saved every N steps (configurable)
- `best_model.pt` - Best model based on validation loss
- `final_model.pt` - Final model after all epochs

Each checkpoint contains:
- Model state dict
- Optimizer state dict
- Scheduler state dict
- Training configuration
- Current epoch and step
- Best loss so far

### Monitoring Training

**With Weights & Biases:**
Visit https://wandb.ai/your-username/EmberNet to see:
- Real-time loss curves
- Learning rate schedules
- Token statistics
- Gradient norms
- Model parameters
- System metrics (GPU/CPU usage, memory)

**Console Output:**
```
======================================================================
Starting Stage 1 Training
======================================================================
Epochs: 3
Steps per epoch: 1000
Total steps: 3000
Batch size: 8
Gradient accumulation: 4
Effective batch size: 32
Learning rate: 0.001
Device: cuda

--- Token Statistics (per sample) ---
Total tokens:  2,048
  â”œâ”€ Image tokens: 64
  â””â”€ Text tokens:  1,984

--- Token Statistics (per batch) ---
Total tokens:  16,384
  â”œâ”€ Image tokens: 512
  â””â”€ Text tokens:  15,872

--- Total Training Tokens (all epochs) ---
Total tokens:  49,152,000
  â”œâ”€ Image tokens: 1,536,000
  â””â”€ Text tokens:  47,616,000
======================================================================

Epoch 1/3 | Step 100 | Loss: 2.3456 | Avg Loss: 2.4567 | LR: 1.00e-03
```

---

## Usage Examples

### Python API

```python
from inference.infer import EmberVLM

# Load model
model = EmberVLM("checkpoints/embernet.pt")

# Basic image Q&A
response = model.chat(
    image="photo.jpg",
    prompt="What's in this image?"
)
print(response)

# OCR - Read text from image
response = model.chat(
    image="document.png",
    prompt="Extract all text from this document"
)

# Chart analysis
response = model.chat(
    image="chart.png",
    prompt="What does this chart show? What's the maximum value?"
)

# Multi-turn conversation (remembers context)
model.chat(image="scene.jpg", prompt="Describe this image")
model.chat(prompt="How many people are there?")  # Uses same image
model.chat(prompt="What are they doing?")        # Continues conversation

# Reset and start fresh
model.clear_history()
```

### Interactive CLI

```bash
python inference/infer.py --interactive

# Commands:
#   /load image.jpg  - Load an image
#   /describe        - Describe the image
#   /ocr             - Extract text
#   /chart           - Analyze as chart
#   /clear           - Reset conversation
#   /quit            - Exit
```

---

## Architecture Details

```
EmberNet VLM (~300M total parameters)
â”‚
â”œâ”€â”€ Vision Encoder: SigLIP-base (FROZEN)
â”‚   â”œâ”€â”€ Parameters: ~85M (not counted in trainable)
â”‚   â”œâ”€â”€ Input: 224Ã—224 RGB image
â”‚   â””â”€â”€ Output: 196 tokens Ã— 768 dims
â”‚
â”œâ”€â”€ Token Compression (TRAINABLE in Stage 1)
â”‚   â”œâ”€â”€ Pixel Shuffle: 196 â†’ 49 tokens
â”‚   â”œâ”€â”€ Adaptive Pooling: 49 â†’ 64 tokens
â”‚   â””â”€â”€ Parameters: ~2M
â”‚
â”œâ”€â”€ Projector (TRAINABLE in Stage 1)
â”‚   â”œâ”€â”€ BitLinear MLP (768 â†’ 768 â†’ 768)
â”‚   â”œâ”€â”€ Ternary weights {-1, 0, +1}
â”‚   â””â”€â”€ Parameters: ~3M
â”‚
â””â”€â”€ Language Decoder: BitNet MoE (TRAINABLE in Stage 2)
    â”œâ”€â”€ Layers: 16 transformer blocks
    â”œâ”€â”€ Hidden size: 768
    â”œâ”€â”€ Attention: GQA (12 heads, 6 KV heads) â€” all projections are BitLinear
    â”œâ”€â”€ MoE FFN:
    â”‚   â”œâ”€â”€ 8 Domain Experts (top-2 routing)
    â”‚   â”‚   â”œâ”€â”€ vision_ocr, vision_diagram
    â”‚   â”‚   â”œâ”€â”€ code_math_chart, code_math_formula
    â”‚   â”‚   â”œâ”€â”€ spatial_reasoning (Ã—2)
    â”‚   â”‚   â””â”€â”€ agentic_reasoning (Ã—2)
    â”‚   â””â”€â”€ 1 Shared Expert (always active)
    â”œâ”€â”€ All weights: Ternary {-1, 0, +1}
    â””â”€â”€ Parameters: ~250M (50M active per forward pass)

Total Trainable: ~255M ternary parameters
Active per Forward: ~55M parameters
Model Size on Disk: <500MB
```

---

## Quantization Implementation Details

### Which modules are ternary vs. full-precision?

| Module | Precision | Notes |
|---|---|---|
| `BitNetAttention` â€” Q/K/V/O projections | **Ternary (1.58-bit)** | All four projections are `BitLinear` |
| `BitNetExpert` â€” gate / up / down projections | **Ternary (1.58-bit)** | All 8 domain experts + shared expert |
| `VisionProjector` â€” fc1, fc2 | **Ternary (1.58-bit)** | `BitLinear` MLP in `models/vision.py` |
| `PixelShuffleCompressor` â€” proj | **FP16** | Standard `nn.Linear`; small (~600K params) |
| `AdaptivePooler` â€” cross-attention | **FP16** | Standard `nn.MultiheadAttention` |
| `RMSNorm` layers (all) | **FP16** | Learnable scale only; ~negligible params |
| Token embeddings + LM head (tied) | **FP16** | `nn.Embedding` + tied `nn.Linear` |
| MoE router (`nn.Linear`) | **FP16** | Small (768 Ã— 8); routing must stay precise |
| SigLIP vision encoder | **FP16, frozen** | Not counted in 255M trainable params |
| VA Refiner MLP classifier | **FP32 (small)** | 2-layer MLP, ~12K params; explicitly not ternary |

> **Summary**: ~250M decoder params (+ ~1M projector) are ternary. Non-quantized
> components (router, layernorms, embeddings, compressor) account for â‰ˆ25M FP16 params.
> The "BitNet-b1.58-style" description is accurate for all learnable weight matrices
> in the decoder and projector.

### How ternary quantization works at runtime

`BitLinear.forward()` in `models/bitnet_moe.py` applies two Straight-Through Estimator (STE) operations:

```python
# Activation quantization (per-token, 8-bit symmetric)
x_quant = x_norm + (activation_quant(x_norm) - x_norm).detach()

# Weight quantization (ternary: sign(W - mean(W)) * mean|W|)
w_quant = w + (weight_quant(w) - w).detach()
```

`weight_quant()` maps float weights to `{-scale, 0, +scale}` using the signed mean:
`u = sign(w - mean(w)) * mean(|w|)`.  The STE means gradients flow through
as if the quantization were identity â€” the underlying float weights are
updated continuously during training and re-quantized at each forward pass.

### On-disk packing (`inference/convert.py`)

After training, `convert_to_ternary()` calls `convert_bitlinear_to_ternary()` on
every `BitLinear` module.  For each:
1. `weight_quant(module.weight)` snaps weights to `{-scale, 0, +scale}`.
2. `pack_ternary_weights()` encodes `{-1, 0, +1}` as `{0b00, 0b01, 0b10}` and
   packs 4 values per byte (2 bits per weight).
3. A FP32 scalar `scale = mean(|w|)` is stored alongside.
4. The resulting `TernaryLinear` module pre-unpacks weights for fast inference.

Non-BitLinear layers (embeddings, layernorms, router, compressor) are quantized
to INT8 via `torch.quantization.quantize_dynamic` in the same pass.

---

## Project Structure

```
EmberNet/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ bitnet_moe.py      # BitLinear + MoE decoder
â”‚   â”œâ”€â”€ vision.py          # SigLIP encoder + compression
â”‚   â”œâ”€â”€ va_refiner.py      # VA Refiner hallucination mitigation
â”‚   â””â”€â”€ vlm.py             # Complete VLM
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ prepare_data.py    # Dataset download script
â”‚   â”œâ”€â”€ data.py            # Data loading
â”‚   â””â”€â”€ train.py           # Training loop
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ convert.py         # Model optimization
â”‚   â””â”€â”€ infer.py           # User interface
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ fig_architecture_overview.py  # Fig 1: architecture + params
â”‚   â”œâ”€â”€ fig_ternary_stats.py          # Fig 2: ternary weight stats
â”‚   â”œâ”€â”€ fig_moe_routing.py            # Fig 3: MoE routing patterns
â”‚   â”œâ”€â”€ fig_latency_energy.py         # Fig 4: latency/energy benchmark
â”‚   â”œâ”€â”€ fig_va_token_effects.py       # Fig 5: VA token-level dynamics
â”‚   â”œâ”€â”€ fig_va_answer_level.py        # Fig 6: VA hallucination metrics
â”‚   â”œâ”€â”€ fig_qualitative_grid.py       # Fig 7: qualitative examples
â”‚   â””â”€â”€ ...                           # existing training-viz scripts
â”œâ”€â”€ generate_all_plots.py  # Master visualization orchestrator
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Hallucination Mitigation (VA Refiner)

EmberNet ships an optional **Visual Absence Refiner (VA Refiner)** that detects and suppresses hallucinated tokens at inference time â€” particularly tokens that describe visual attributes (colors, objects, counts, spatial relations) that are not grounded in the image.

### How it works

The VA Refiner operates in three layers:

1. **Neuron-level monitoring** â€” Forward hooks are placed on `shared_expert.down_proj` in the BitNet MoE layers specified by `va_layer_indices` (default: layers 6â€“11). The top-K neuron activations are fed to a lightweight 2-layer MLP that predicts a per-step "visual hallucination score" `p_neuron âˆˆ [0, 1]`.

2. **Logit discrepancy** â€” A null baseline is computed once at the start of generation by running the decoder with image tokens zeroed out. At each step the L1 distance between the live logit distribution and the null baseline is collapsed to a score `p_logit = 1 âˆ’ tanh(dist / scale)`. High similarity to the null baseline means the model is generating as if there were no image â€” a sign of hallucination.

3. **Temporal burst detection** â€” A sliding window tracks the blended score `p = Î±Â·p_neuron + (1âˆ’Î±)Â·p_logit`. If the window mean exceeds `va_burst_threshold`, a "burst" is declared and a soft log-prob penalty (`va_soft_penalty`) is added to every token in `VISUAL_KEYWORDS` until the window cools.

After generation, `calibrate_answer_prefix()` checks the mean VA score for the response. If it exceeds 0.70, the response is prefixed with *"I cannot see that clearly, but â€¦"*. If it exceeds 0.40, the prefix is *"I might be wrong, but â€¦"*.

### CLI usage

```bash
# Single inference with VA Refiner
python inference/infer.py \
  --model checkpoints/embernet.pt \
  --image photo.jpg \
  --prompt "What color is the car?" \
  --use-va-refiner \
  --va-threshold 0.70 \
  --va-burst-threshold 0.70 \
  --va-soft-penalty 5.0 \
  --va-alpha 0.5
```

### Python API

```python
from inference.infer import EmberVLM

model = EmberVLM(
    model_path="checkpoints/embernet.pt",
    use_va_refiner=True,
    va_threshold=0.70,
    va_burst_threshold=0.70,
    va_soft_penalty=5.0,
    va_alpha=0.5,
)
response = model.chat(image="photo.jpg", prompt="What color is the car?")
print(response)
```

### Configuration knobs

| Argument | Default | Description |
|---|---|---|
| `--use-va-refiner` | off | Enable VA Refiner |
| `--va-threshold` | 0.70 | VA score threshold per token (non-visual) |
| `--va-burst-threshold` | 0.70 | Window mean that triggers burst mode |
| `--va-soft-penalty` | 5.0 | Log-prob penalty during bursts |
| `--va-alpha` | 0.5 | Blend: 1.0 = pure neuron, 0.0 = pure logit discrepancy |

Fine-grained knobs (`va_layer_indices`, `va_neuron_k`, `va_window_size`, `va_decay_factor`, `va_logit_scale`) can be set programmatically via `VARefinerConfig` in `models/va_refiner.py`.

### Trade-offs

- **Overhead**: one extra decoder forward pass (null baseline) at generation start, plus per-step hook collection. On a single A100 this adds ~5â€“10% latency per response.
- **Conservative by design**: the refiner penalises rather than blocks tokens, so factual accuracy is preserved. Use `va_soft_penalty < 3.0` for a lighter touch.

---

## Visualization Suite

EmberNet ships a publication-grade visualization suite in `visualizations/`.
Generate all figures or individual ones via `generate_all_plots.py`.

### Quick commands

```bash
# Generate ALL paper figures (Figs 1â€“7, synthetic data, no model required)
python generate_all_plots.py --paper-only

# Generate ALL plots (training-viz + paper figures)
python generate_all_plots.py --all

# Single paper figure
python generate_all_plots.py --fig fig_ternary_stats

# With a real checkpoint for data-driven figures
python generate_all_plots.py --paper-only --model checkpoints/stage2/final_model.pt
```

### Figure catalogue

| Figure | Script | Description |
|---|---|---|
| Fig 1 | `fig_architecture_overview.py` | Pipeline block diagram + parameter/bitwidth breakdown per component |
| Fig 2 | `fig_ternary_stats.py` | Per-layer ternary weight sparsity heatmaps and {-1,0,+1} composition bars |
| Fig 3 | `fig_moe_routing.py` | MoE expert routing frequency matrix across vision-language datasets |
| Fig 4 | `fig_latency_energy.py` | Latency, energy, and throughput: ternary vs FP16 baseline (Â±std bars) |
| Fig 5 | `fig_va_token_effects.py` | Per-token p_VA trajectories with burst-mode and penalisation markers |
| Fig 6 | `fig_va_answer_level.py` | Answer-level hallucination rate with/without VA Refiner, by visual category |
| Fig 7 | `fig_qualitative_grid.py` | Multi-domain qualitative panel: baseline vs VA-refined answers + p_VA sparklines |

All figures are saved as both `.pdf` (vector, for LaTeX) and `.png` (300 DPI) in `plots/paper_figures/`.

---

## License

MIT License

